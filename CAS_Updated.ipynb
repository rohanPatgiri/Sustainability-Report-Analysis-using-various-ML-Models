{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install PyPDF2\n",
        "!pip install pycryptodome"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p7YJRAjKwRTH",
        "outputId": "3955dd2a-8152-4c83-c99f-2382058cb2e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/232.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━\u001b[0m \u001b[32m225.3/232.6 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-3.0.1\n",
            "Collecting pycryptodome\n",
            "  Downloading pycryptodome-3.20.0-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pycryptodome\n",
            "Successfully installed pycryptodome-3.20.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IChXH7yDJ5XC",
        "outputId": "5e6906e1-fc79-4f65-cbae-08ce4685bdfa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 1"
      ],
      "metadata": {
        "id": "_L-41oXGSGJ1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dgAkoxvhQvgi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45fb5e68-1500-4b20-ae30-f892bc161ba3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                           Sentences\n",
            "0  SUSTAINABILITY REPORT 2020\\nROYAL DUTCH SHELL ...\n",
            "1  Our strategy to accelerate the \\ntransition to...\n",
            "2  Powering Progress is designed to create value ...\n",
            "3  The \\nstrategy seeks to accelerate Shell’s tra...\n",
            "4  Powering Progress is designed to create value ...\n",
            "Tokenized sentences have been saved to: /content/drive/MyDrive/Colab Notebooks/shell_2020.csv\n"
          ]
        }
      ],
      "source": [
        "### Step 1: Extract text from PDf and store it in a database\n",
        "\n",
        "from PyPDF2 import PdfReader\n",
        "import pandas as pd\n",
        "from nltk.tokenize import sent_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "def tokenize_sentences(text):\n",
        "    sentences = sent_tokenize(text)\n",
        "    return sentences\n",
        "\n",
        "def process_pdf(pdf_path):\n",
        "    reader = PdfReader(pdf_path)\n",
        "    num_pages = len(reader.pages)\n",
        "\n",
        "    # Initialize an empty DataFrame to store the results\n",
        "    df_combined = pd.DataFrame(columns=['Sentences'])\n",
        "\n",
        "    for page_number in range(1, num_pages + 1):\n",
        "        # Extract text from each page\n",
        "        page = reader.pages[page_number - 1]\n",
        "        extracted_text = page.extract_text()\n",
        "\n",
        "        # Tokenize sentences using NLTK\n",
        "        sentences = tokenize_sentences(extracted_text)\n",
        "\n",
        "        # Create a DataFrame with the sentences\n",
        "        df_page = pd.DataFrame({'Sentences': sentences})\n",
        "\n",
        "        # Concatenate the current page's DataFrame to the combined DataFrame\n",
        "        df_combined = pd.concat([df_combined, df_page], ignore_index=True)\n",
        "\n",
        "    return df_combined\n",
        "\n",
        "# Specify the path for the PDF file\n",
        "pdf_path = '/content/drive/MyDrive/Colab Notebooks/shell-sustainability-report-2020_unlocked.pdf'\n",
        "\n",
        "# Process the PDF and get the combined DataFrame\n",
        "df_result = process_pdf(pdf_path)\n",
        "\n",
        "# Specify the path for the final CSV file\n",
        "csv_path = '/content/drive/MyDrive/Colab Notebooks/shell_2020.csv'\n",
        "\n",
        "# Save the combined DataFrame to a CSV file\n",
        "df_result.to_csv(csv_path, index=False)\n",
        "\n",
        "# Display the combined DataFrame\n",
        "print(df_result.head())\n",
        "print(f\"Tokenized sentences have been saved to: {csv_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2"
      ],
      "metadata": {
        "id": "cisT2foiSIm1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "###  Step 2: Truncate (shorten) the sentences to 200 words or less\n",
        "import pandas as pd\n",
        "\n",
        "def process_text(text):\n",
        "    max_words_per_chunk = 200\n",
        "    words = text.split()\n",
        "    chunks = [words[i:i + max_words_per_chunk] for i in range(0, len(words), max_words_per_chunk)]\n",
        "    return chunks\n",
        "\n",
        "def process_csv(input_csv_path, output_csv_path):\n",
        "    # Read the CSV file\n",
        "    df = pd.read_csv(input_csv_path)\n",
        "\n",
        "    # Create a new DataFrame for the updated data\n",
        "    updated_data = []\n",
        "\n",
        "    # Process each row in the first column\n",
        "    for index, row in df.iterrows():\n",
        "        text = str(row.iloc[0])  # Assuming the first column is the relevant one\n",
        "        words_count = len(text.split())\n",
        "        if words_count > 200:\n",
        "            chunks = process_text(text)\n",
        "            for chunk in chunks:\n",
        "                updated_data.append([chunk])\n",
        "\n",
        "    # Create a new DataFrame with the processed data\n",
        "    updated_df = pd.DataFrame(updated_data, columns=[df.columns[0]])\n",
        "\n",
        "    # Concatenate the original DataFrame and the updated DataFrame\n",
        "    final_df = pd.concat([df, updated_df], ignore_index=True)\n",
        "\n",
        "    # Save the updated DataFrame to a new CSV file\n",
        "    final_df.to_csv(output_csv_path, index=False)\n",
        "\n",
        "# Replace 'input.csv' and 'output.csv' with your actual file paths\n",
        "process_csv('/content/drive/MyDrive/Colab Notebooks/shell_2020.csv', '/content/drive/MyDrive/Colab Notebooks/shell_2020_output.csv')"
      ],
      "metadata": {
        "id": "A9JF_kgaQ5Md"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 3\n"
      ],
      "metadata": {
        "id": "I8GcxkK_SKiN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Step 3 : Generate E, S and G score for the report\n",
        "\n",
        "import pandas as pd\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, pipeline\n",
        "import torch\n",
        "\n",
        "# Load the ESG scoring model from Hugging Face\n",
        "model_name = \"yiyanghkust/finbert-esg\"\n",
        "model = BertForSequenceClassification.from_pretrained(model_name)\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "esg_pipeline = pipeline(\"text-classification\", model=model, tokenizer=tokenizer)\n",
        "\n",
        "# Function to get E, S, G scores for a sentence\n",
        "def get_esg_scores(sentence):\n",
        "    try:\n",
        "        scores = esg_pipeline(sentence)\n",
        "        return {score['label']: score['score'] for score in scores}\n",
        "    except RuntimeError as e:\n",
        "        print(f\"Error for sentence. Skipping row.\")\n",
        "        return None\n",
        "\n",
        "# Function to calculate average scores from a DataFrame\n",
        "def calculate_average_scores(df):\n",
        "    average_scores = {}\n",
        "    for label in ['Environmental', 'Social', 'Governance']:\n",
        "        column_name = f'{label}_Score'\n",
        "        non_blank_entries = df[column_name].count()\n",
        "        sum_scores = df[column_name].sum(skipna=True)\n",
        "        average_score = sum_scores / non_blank_entries if non_blank_entries > 0 else 0\n",
        "        average_scores[label] = average_score\n",
        "    return average_scores\n",
        "\n",
        "# Function to process CSV and calculate average scores\n",
        "def process_csv_and_calculate_average(csv_path):\n",
        "    # Load the CSV file\n",
        "    df = pd.read_csv(csv_path)\n",
        "\n",
        "    # Iterate over each row and update E, S, G scores\n",
        "    for index, row in df.iterrows():\n",
        "        sentence = row['Sentences']  # Replace with the actual column name\n",
        "        scores = get_esg_scores(sentence)\n",
        "\n",
        "        if scores is not None:\n",
        "            for label, score in scores.items():\n",
        "                df.at[index, f'{label}_Score'] = score\n",
        "\n",
        "    # Calculate and return average scores\n",
        "    return calculate_average_scores(df)\n",
        "\n",
        "# Replace 'input.csv' with your actual file path\n",
        "csv_path = '/content/drive/MyDrive/Colab Notebooks/shell_2020_output.csv'\n",
        "average_scores = process_csv_and_calculate_average(csv_path)\n",
        "\n",
        "# Print average scores\n",
        "print(\"Average E Score:\", average_scores['Environmental'])\n",
        "print(\"Average S Score:\", average_scores['Social'])\n",
        "print(\"Average G Score:\", average_scores['Governance'])\n"
      ],
      "metadata": {
        "id": "y2VJ17FbRDgs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8fbff24b-b999-4da8-ea3b-fa9f4fbd7e24"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error for sentence. Skipping row.\n",
            "Error for sentence. Skipping row.\n",
            "Error for sentence. Skipping row.\n",
            "Error for sentence. Skipping row.\n",
            "Error for sentence. Skipping row.\n",
            "Error for sentence. Skipping row.\n",
            "Error for sentence. Skipping row.\n",
            "Error for sentence. Skipping row.\n",
            "Error for sentence. Skipping row.\n",
            "Error for sentence. Skipping row.\n",
            "Error for sentence. Skipping row.\n",
            "Error for sentence. Skipping row.\n",
            "Error for sentence. Skipping row.\n",
            "Error for sentence. Skipping row.\n",
            "Error for sentence. Skipping row.\n",
            "Error for sentence. Skipping row.\n",
            "Error for sentence. Skipping row.\n",
            "Error for sentence. Skipping row.\n",
            "Error for sentence. Skipping row.\n",
            "Error for sentence. Skipping row.\n",
            "Error for sentence. Skipping row.\n",
            "Error for sentence. Skipping row.\n",
            "Error for sentence. Skipping row.\n",
            "Error for sentence. Skipping row.\n",
            "Error for sentence. Skipping row.\n",
            "Error for sentence. Skipping row.\n",
            "Error for sentence. Skipping row.\n",
            "Error for sentence. Skipping row.\n",
            "Error for sentence. Skipping row.\n",
            "Error for sentence. Skipping row.\n",
            "Error for sentence. Skipping row.\n",
            "Error for sentence. Skipping row.\n",
            "Error for sentence. Skipping row.\n",
            "Error for sentence. Skipping row.\n",
            "Error for sentence. Skipping row.\n",
            "Error for sentence. Skipping row.\n",
            "Average E Score: 0.9301571789073481\n",
            "Average S Score: 0.8929667209261983\n",
            "Average G Score: 0.7887478679418564\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 4 : Identifying main topics and trends using techniques like LDA (Latent Dirichlet Allocation)."
      ],
      "metadata": {
        "id": "2Ur-pV4binAl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from bertopic import BERTopic\n",
        "import pandas as pd\n",
        "\n",
        "# Load the model\n",
        "topic_model = BERTopic.load(\"MaartenGr/BERTopic_Wikipedia\")\n",
        "\n",
        "# Get topic information\n",
        "topic_info = topic_model.get_topic_info()\n",
        "\n",
        "# Convert to pandas DataFrame\n",
        "df2 = pd.DataFrame(topic_info)\n",
        "\n",
        "##..................\n",
        "import pandas as pd\n",
        "from bertopic import BERTopic\n",
        "\n",
        "# Load the BERTopic model\n",
        "topic_model = BERTopic.load(\"MaartenGr/BERTopic_Wikipedia\")\n",
        "\n",
        "def get_topic_info(topic_model, sentence):\n",
        "    # Predict topic assignments for the sentence\n",
        "    z = topic_model.transform(sentence)\n",
        "    # Get topic number and confidence level\n",
        "    topic_number = z[0][0]\n",
        "    confidence_level = z[1][0]\n",
        "    # Get topic name\n",
        "    topic_name = df2.loc[df2['Topic'] == topic_number, 'Name'].iloc[0]\n",
        "    return topic_number, confidence_level, topic_name\n",
        "\n",
        "# Read the CSV file\n",
        "input_csv_file = '/content/drive/MyDrive/2024/CAS/Truncated1.csv'  # Replace with your input CSV file\n",
        "output_csv_file = '/content/drive/MyDrive/2024/CAS/outputLDA.csv'  # Specify the output CSV file\n",
        "\n",
        "df = pd.read_csv(input_csv_file)\n",
        "\n",
        "# Iterate over each row (excluding the header)\n",
        "for index, row in df.iterrows():\n",
        "    if index == 0:  # Skip the header row\n",
        "        continue\n",
        "    # Get the sentence from the first column\n",
        "    sentence = row['Sentences']\n",
        "    # Get topic info\n",
        "    topic_number, confidence_level, topic_name = get_topic_info(topic_model, sentence)\n",
        "    # Store topic info in new columns\n",
        "    df.at[index, 'Topic Number'] = topic_number\n",
        "    df.at[index, 'Confidence Level'] = confidence_level\n",
        "    df.at[index, 'Topic Name'] = topic_name\n",
        "\n",
        "# Save the DataFrame to a new CSV file\n",
        "df.to_csv(output_csv_file, index=False)"
      ],
      "metadata": {
        "id": "fUfng3htPhNc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 5.1 : Calculate the frequencies of \"Topic Name\" and sort them"
      ],
      "metadata": {
        "id": "VU3LOmwyjI39"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Up Next, Calculate the frequencies of \"Name\" and sort them\n",
        "import pandas as pd\n",
        "\n",
        "# Read the CSV file\n",
        "input_csv_file = 'outputLDA.csv'  # Replace with your input CSV file\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/2024/CAS/outputLDA.csv\")\n",
        "\n",
        "# Calculate the frequency of occurrence of each unique entry in the \"Topic Name\" column\n",
        "topic_name_counts = df['Topic Name'].value_counts()\n",
        "\n",
        "# Get the top 5 frequencies\n",
        "top_2_topics = topic_name_counts.head(2)\n",
        "\n",
        "# Print the top 5 frequencies along with the corresponding values in the \"Confidence Level\" column\n",
        "print(\"Top 2 Topic Name Frequencies with Corresponding Confidence Levels:\")\n",
        "for topic_name, frequency in top_5_topics.items():\n",
        "    confidence_level = df[df['Topic Name'] == topic_name]['Confidence Level'].iloc[0]\n",
        "    print(f\"Topic Name: {topic_name}, Frequency: {frequency}, Confidence Level: {confidence_level}\")"
      ],
      "metadata": {
        "id": "Rz40FdzRjCnz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 5.2: Store the most relevant \"Topic names\" in a single Python list"
      ],
      "metadata": {
        "id": "FfUNbRKvjbpl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_words = []\n",
        "\n",
        "# Iterate over each topic name in the top 5 topics\n",
        "for topic_name, frequency in top_5_topics.items():\n",
        "    # Split the topic name based on the \"_\" delimiter and filter out numerical values\n",
        "    words = [word for word in topic_name.split(\"_\") if not word.isdigit()]\n",
        "    # Extend the list of all words with the words from the current topic name\n",
        "    all_words.extend(words)\n",
        "\n",
        "# Print the merged list of all words\n",
        "print(\"Merged List of All Words:\")\n",
        "print(all_words)"
      ],
      "metadata": {
        "id": "CUa8gHZ7jTkD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 6: Thematic Analysis"
      ],
      "metadata": {
        "id": "huo-uMiWjlyD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from transformers import pipeline\n",
        "\n",
        "# Load the zero-shot classification pipeline\n",
        "classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
        "\n",
        "# Read the CSV file\n",
        "input_csv_file = '/content/drive/MyDrive/2024/CAS/Truncated2021.csv'  # Replace with your input CSV file\n",
        "output_csv_file = '/content/drive/MyDrive/2024/CAS/Thematic2021.csv'  # Specify the output CSV file\n",
        "\n",
        "# List of keywords\n",
        "\n",
        "\n",
        "# Initialize an empty list to store the results\n",
        "results = []\n",
        "\n",
        "# Read the CSV file and iterate over each row\n",
        "df = pd.read_csv(input_csv_file)\n",
        "count = 1\n",
        "for index, row in df.iterrows():\n",
        "    # Get the sentence from the \"Sentences\" column\n",
        "    sentence = row['Sentences']\n",
        "\n",
        "    # Perform zero-shot classification\n",
        "    scores = classifier(sentence, all_words)['scores']\n",
        "\n",
        "    # Store the scores in a dictionary\n",
        "    result = {'Sentence': sentence}\n",
        "    for label, score in zip(all_words, scores):\n",
        "        result[label] = score\n",
        "\n",
        "    # Append the result to the list\n",
        "    results.append(result)\n",
        "\n",
        "    # Print the scores for each keyword\n",
        "    print (\"SENTENCE NO. \", count)\n",
        "    print(f\"Sentence: {sentence}\")\n",
        "    for label, score in zip(all_words, scores):\n",
        "        print(f\"{label}: {score}\")\n",
        "    count+=1\n",
        "# Convert the list of results to a DataFrame\n",
        "output_df = pd.DataFrame(results)\n",
        "\n",
        "# Save the DataFrame to a new CSV file\n",
        "output_df.to_csv(output_csv_file, index=False)"
      ],
      "metadata": {
        "id": "SSaKb0nzjlOz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}